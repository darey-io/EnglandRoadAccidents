{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#Set up Spark Configuration ==> To be executed only once though, hence its in a commented block\n",
    "'''conf = SparkConf().setAppName(\"EnglandAccidents\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlCtx = SQLContext(sc)'''\n",
    "\n",
    "#set up the function to clean up some dirty data\n",
    "\n",
    "def cleanFields(fields):\n",
    "    accFields = fields\n",
    "    \n",
    "    counter = 0\n",
    "    for index, field in enumerate(accFields):  #Use enumerate function to create a key value pair for index and field \n",
    "\t\t#Clean up occurences of 1st and Second\n",
    "\t\tif \"1st\" in field:\t\t\t\t\t   \n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[:3], \"first\")  #Replace '1st' with \"First\" within the substring\n",
    "\t\t\taccFields[index] = newstr  \t\t\t\t\t  #Set the new values\n",
    "\t\tif \"2nd\" in field:\n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[:3], \"Second\")\n",
    "\t\t\taccFields[index] = newstr\n",
    "\t\t#Clean up occurences of parenthesis\n",
    "\t\tif \"(\" in field:\t\t\t\t\t\t\t#If there is an open bracket, then there must be a closing one\n",
    "\t\t\t#print (field)\n",
    "\t\t\tfor indx, i in enumerate(field):\t\t#for each letter in the selected field (field Or column, dont be confused, both are used interchangeably\n",
    "\t\t\t\tif i == \"(\":\n",
    "\t\t\t\t\tbracketIndexOpen = indx         #Get the index of the Open parenthesis\n",
    "\t\t\toldstr = field\n",
    "\t\t\t#print(oldstr)\n",
    "\t\t\tnewstrOpenBrac = oldstr.replace(oldstr[bracketIndexOpen], \"\")  #the replaced version of open parenthesis\n",
    "\t\t\t#print (newstrOpenBrac)\t\n",
    "\t\t\tfor idx , i in enumerate(newstrOpenBrac):\t\t\t\t\t\t#Loop through the replaced version so that we can get the updated index for close parenthesis\n",
    "\t\t\t\tlatestCloseindex = idx\t\n",
    "\t\t\t\tnewstrCloseBrac = newstrOpenBrac.replace(newstrOpenBrac[latestCloseindex], \"\")  #Now remove close parenthesis \n",
    "\t\t\t#print(newstrCloseBrac)\n",
    "\t\t\taccFields[index] = newstrCloseBrac\n",
    "\t\t\t#Clean out occurences of \"-\". Same Logic above applies. I know that there is only 1 occurence per field, if there were more, the logic would be different\n",
    "\t\tif \"-\" in field:\t\t\t\t\t\t\t\n",
    "\t\t\t#print (field)\n",
    "\t\t\tfor indx, i in enumerate(field):\t\t\n",
    "\t\t\t\tif i == \"-\":\n",
    "\t\t\t\t\thyphenIndex = indx \n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[hyphenIndex], \"_\")\n",
    "\t\t\taccFields[index] = newstr\n",
    "\n",
    "#Columns extracted from our source CSV data \n",
    "accFields = ['Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles',\\\n",
    "'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', \\\n",
    "'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities',\\\n",
    "'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', \\\n",
    "'Did_Police_Officer_Attend_Scene_of_Accident', 'LSOA_of_Accident_Location']\n",
    "\n",
    "\n",
    "#Clean up time\n",
    "cleanFields(accFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the raw data into an RDD, and separate each field based on \",\"\n",
    "acc= sc.textFile('/Users/dare.olufunmilayo/Home_Dare\\'sProjects/EnglandRoadAccidents/data/Accidents_2014.csv')\n",
    "acc= acc.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Accident_Index',\n",
       "  u'Location_Easting_OSGR',\n",
       "  u'Location_Northing_OSGR',\n",
       "  u'Longitude',\n",
       "  u'Latitude',\n",
       "  u'Police_Force',\n",
       "  u'Accident_Severity',\n",
       "  u'Number_of_Vehicles',\n",
       "  u'Number_of_Casualties',\n",
       "  u'Date',\n",
       "  u'Day_of_Week',\n",
       "  u'Time',\n",
       "  u'Local_Authority_(District)',\n",
       "  u'Local_Authority_(Highway)',\n",
       "  u'1st_Road_Class',\n",
       "  u'1st_Road_Number',\n",
       "  u'Road_Type',\n",
       "  u'Speed_limit',\n",
       "  u'Junction_Detail',\n",
       "  u'Junction_Control',\n",
       "  u'2nd_Road_Class',\n",
       "  u'2nd_Road_Number',\n",
       "  u'Pedestrian_Crossing-Human_Control',\n",
       "  u'Pedestrian_Crossing-Physical_Facilities',\n",
       "  u'Light_Conditions',\n",
       "  u'Weather_Conditions',\n",
       "  u'Road_Surface_Conditions',\n",
       "  u'Special_Conditions_at_Site',\n",
       "  u'Carriageway_Hazards',\n",
       "  u'Urban_or_Rural_Area',\n",
       "  u'Did_Police_Officer_Attend_Scene_of_Accident',\n",
       "  u'LSOA_of_Accident_Location']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a sense of what the columns look like. You can see we have some not so nice columns there. \n",
    "#For example... '2nd_Road_Class' should'nt start with a number\n",
    "acc.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map the fields with a Class on the fly using namedtuple\n",
    "accNamedTuples = namedtuple('Accidents', accFields)\n",
    "\n",
    "#DATE_FMT = \"%d/%m/%Y\"  \n",
    "#TIME_FMT = \"%H%M\"\n",
    "\n",
    "#Define a function that will help us to map each field to a column within the RDD (I still need to work out how to convert to time based types)\n",
    "def parse(row):\n",
    "\t#row[9] = datetime.datetime.strptime(row[9], DATE_FMT).date()   #Date\n",
    "\t#row[11] = datetime.datetime.strptime(row[11], TIME_FMT).time() #Time\n",
    "\treturn accNamedTuples(*row[:32])\n",
    "\n",
    "#Map data with columns and filter out the header\n",
    "parsedAccData= acc.map(parse)\n",
    "\n",
    "#Optionally you may want to filter out the header\n",
    "accData = parsedAccData.filter(lambda x: 'Date' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Accidents(Accident_Index=u'201401BS70001', Location_Easting_OSGR=u'524600', Location_Northing_OSGR=u'179020', Longitude=u'-0.206443', Latitude=u'51.496345', Police_Force=u'1', Accident_Severity=u'3', Number_of_Vehicles=u'2', Number_of_Casualties=u'1', Date=u'09/01/2014', Day_of_Week=u'5', Time=u'13:21', Local_Authority_District=u'12', Local_Authority_Highway=u'E09000020', first_Road_Class=u'3', first_Road_Number=u'315', Road_Type=u'6', Speed_limit=u'30', Junction_Detail=u'0', Junction_Control=u'-1', Second_Road_Class=u'-1', Second_Road_Number=u'0', Pedestrian_Crossing_Human_Control=u'0', Pedestrian_Crossing_Physical_Facilities=u'0', Light_Conditions=u'1', Weather_Conditions=u'2', Road_Surface_Conditions=u'2', Special_Conditions_at_Site=u'0', Carriageway_Hazards=u'0', Urban_or_Rural_Area=u'1', Did_Police_Officer_Attend_Scene_of_Accident=u'2', LSOA_of_Accident_Location=u'E01002814')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at our new column structure\n",
    "accData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Date', u'Weather_Conditions'),\n",
       " (u'09/01/2014', u'2'),\n",
       " (u'20/01/2014', u'1'),\n",
       " (u'21/01/2014', u'1'),\n",
       " (u'15/01/2014', u'1'),\n",
       " (u'09/01/2014', u'1'),\n",
       " (u'17/01/2014', u'1'),\n",
       " (u'10/01/2014', u'1'),\n",
       " (u'30/01/2014', u'2'),\n",
       " (u'10/01/2014', u'1')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOu can pick out columns that you want from the data\n",
    "parsedAccData.map(lambda x: (x.Date, x.Weather_Conditions)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#areas to explore ==>\n",
    "# Relationship between accidents and road conditions\n",
    "# Show the trend for"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
