{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#Set up Spark Configuration ==> To be executed only once though, hence its in a commented block\n",
    "'''conf = SparkConf().setAppName(\"EnglandAccidents\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlCtx = SQLContext(sc)'''\n",
    "\n",
    "#set up the function to clean up some dirty data\n",
    "\n",
    "def cleanFields(fields):\n",
    "    accFields = fields\n",
    "    \n",
    "    counter = 0\n",
    "    for index, field in enumerate(accFields):  #Use enumerate function to create a key value pair for index and field \n",
    "\t\t#Clean up occurences of 1st and Second\n",
    "\t\tif \"1st\" in field:\t\t\t\t\t   \n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[:3], \"first\")  #Replace '1st' with \"First\" within the substring\n",
    "\t\t\taccFields[index] = newstr  \t\t\t\t\t  #Set the new values\n",
    "\t\tif \"2nd\" in field:\n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[:3], \"Second\")\n",
    "\t\t\taccFields[index] = newstr\n",
    "\t\t#Clean up occurences of parenthesis\n",
    "\t\tif \"(\" in field:\t\t\t\t\t\t\t#If there is an open bracket, then there must be a closing one\n",
    "\t\t\t#print (field)\n",
    "\t\t\tfor indx, i in enumerate(field):\t\t#for each letter in the selected field (field Or column, dont be confused, both are used interchangeably\n",
    "\t\t\t\tif i == \"(\":\n",
    "\t\t\t\t\tbracketIndexOpen = indx         #Get the index of the Open parenthesis\n",
    "\t\t\toldstr = field\n",
    "\t\t\t#print(oldstr)\n",
    "\t\t\tnewstrOpenBrac = oldstr.replace(oldstr[bracketIndexOpen], \"\")  #the replaced version of open parenthesis\n",
    "\t\t\t#print (newstrOpenBrac)\t\n",
    "\t\t\tfor idx , i in enumerate(newstrOpenBrac):\t\t\t\t\t\t#Loop through the replaced version so that we can get the updated index for close parenthesis\n",
    "\t\t\t\tlatestCloseindex = idx\t\n",
    "\t\t\t\tnewstrCloseBrac = newstrOpenBrac.replace(newstrOpenBrac[latestCloseindex], \"\")  #Now remove close parenthesis \n",
    "\t\t\t#print(newstrCloseBrac)\n",
    "\t\t\taccFields[index] = newstrCloseBrac\n",
    "\t\t\t#Clean out occurences of \"-\". Same Logic above applies. I know that there is only 1 occurence per field, if there were more, the logic would be different\n",
    "\t\tif \"-\" in field:\t\t\t\t\t\t\t\n",
    "\t\t\t#print (field)\n",
    "\t\t\tfor indx, i in enumerate(field):\t\t\n",
    "\t\t\t\tif i == \"-\":\n",
    "\t\t\t\t\thyphenIndex = indx \n",
    "\t\t\toldstr = field\n",
    "\t\t\tnewstr = oldstr.replace(oldstr[hyphenIndex], \"_\")\n",
    "\t\t\taccFields[index] = newstr\n",
    "\n",
    "#Columns extracted from our source CSV data \n",
    "accFields = ['Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles',\\\n",
    "'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', \\\n",
    "'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities',\\\n",
    "'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', \\\n",
    "'Did_Police_Officer_Attend_Scene_of_Accident', 'LSOA_of_Accident_Location']\n",
    "\n",
    "\n",
    "#Clean up time\n",
    "cleanFields(accFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the raw data into an RDD, and separate each field based on \",\"\n",
    "acc= sc.textFile('/Users/dare.olufunmilayo/Home_Dare\\'sProjects/EnglandRoadAccidents/data/Accidents_2014.csv')\n",
    "acc= acc.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Accident_Index',\n",
       "  u'Location_Easting_OSGR',\n",
       "  u'Location_Northing_OSGR',\n",
       "  u'Longitude',\n",
       "  u'Latitude',\n",
       "  u'Police_Force',\n",
       "  u'Accident_Severity',\n",
       "  u'Number_of_Vehicles',\n",
       "  u'Number_of_Casualties',\n",
       "  u'Date',\n",
       "  u'Day_of_Week',\n",
       "  u'Time',\n",
       "  u'Local_Authority_(District)',\n",
       "  u'Local_Authority_(Highway)',\n",
       "  u'1st_Road_Class',\n",
       "  u'1st_Road_Number',\n",
       "  u'Road_Type',\n",
       "  u'Speed_limit',\n",
       "  u'Junction_Detail',\n",
       "  u'Junction_Control',\n",
       "  u'2nd_Road_Class',\n",
       "  u'2nd_Road_Number',\n",
       "  u'Pedestrian_Crossing-Human_Control',\n",
       "  u'Pedestrian_Crossing-Physical_Facilities',\n",
       "  u'Light_Conditions',\n",
       "  u'Weather_Conditions',\n",
       "  u'Road_Surface_Conditions',\n",
       "  u'Special_Conditions_at_Site',\n",
       "  u'Carriageway_Hazards',\n",
       "  u'Urban_or_Rural_Area',\n",
       "  u'Did_Police_Officer_Attend_Scene_of_Accident',\n",
       "  u'LSOA_of_Accident_Location']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a sense of what the columns look like. You can see we have some not so nice columns there. \n",
    "#For example... '2nd_Road_Class' should'nt start with a number\n",
    "acc.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map the fields with a Class on the fly using namedtuple\n",
    "accNamedTuples = namedtuple('Accidents', accFields)\n",
    "\n",
    "#DATE_FMT = \"%d/%m/%Y\"  \n",
    "#TIME_FMT = \"%H%M\"\n",
    "\n",
    "#Define a function that will help us to map each field to a column within the RDD (I still need to work out how to convert to time based types)\n",
    "def parse(row):\n",
    "\t#row[9] = datetime.datetime.strptime(row[9], DATE_FMT).date()   #Date\n",
    "\t#row[11] = datetime.datetime.strptime(row[11], TIME_FMT).time() #Time\n",
    "\treturn accNamedTuples(*row[:32])\n",
    "\n",
    "#Map data with columns and filter out the header\n",
    "parsedAccData= acc.map(parse)\n",
    "\n",
    "#Optionally you may want to filter out the header\n",
    "accData = parsedAccData.filter(lambda x: 'Date' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Accidents(Accident_Index=u'201401BS70001', Location_Easting_OSGR=u'524600', Location_Northing_OSGR=u'179020', Longitude=u'-0.206443', Latitude=u'51.496345', Police_Force=u'1', Accident_Severity=u'3', Number_of_Vehicles=u'2', Number_of_Casualties=u'1', Date=u'09/01/2014', Day_of_Week=u'5', Time=u'13:21', Local_Authority_District=u'12', Local_Authority_Highway=u'E09000020', first_Road_Class=u'3', first_Road_Number=u'315', Road_Type=u'6', Speed_limit=u'30', Junction_Detail=u'0', Junction_Control=u'-1', Second_Road_Class=u'-1', Second_Road_Number=u'0', Pedestrian_Crossing_Human_Control=u'0', Pedestrian_Crossing_Physical_Facilities=u'0', Light_Conditions=u'1', Weather_Conditions=u'2', Road_Surface_Conditions=u'2', Special_Conditions_at_Site=u'0', Carriageway_Hazards=u'0', Urban_or_Rural_Area=u'1', Did_Police_Officer_Attend_Scene_of_Accident=u'2', LSOA_of_Accident_Location=u'E01002814')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at our new column structure\n",
    "accData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Date', u'Day_of_Week'),\n",
       " (u'09/01/2014', u'5'),\n",
       " (u'20/01/2014', u'2'),\n",
       " (u'21/01/2014', u'3'),\n",
       " (u'15/01/2014', u'4')]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOu can pick out columns that you want from the data\n",
    "accDow = parsedAccData.map(lambda x: (x.Date, x.Day_of_Week))\n",
    "#Print out the new columns\n",
    "accDow.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going on from here are areas to explore  \n",
    "\n",
    "#The day of the week with the highest rate of accidents\n",
    "\n",
    "# Relationship between accidents and road conditions\n",
    "# Show the trend for and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' Sunday'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need another RDD to be able to lookup what the numbers representing DOW mean\n",
    "ISO_dow =sc.textFile('/Users/dare.olufunmilayo/Home_Dare\\'sProjects/EnglandRoadAccidents/data/ISO_dow.csv')\n",
    "dow =sc.textFile('/Users/dare.olufunmilayo/Home_Dare\\'sProjects/EnglandRoadAccidents/data/dow.csv')\n",
    "lookupDow = dow.map(lambda x: x.split(',')).collectAsMap()\n",
    "lookupDow['6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' Sunday'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This Data is using the ISO Format\n",
    "lookupISO_dow = ISO_dow.map(lambda x : x.split(',')).collectAsMap()\n",
    "lookupISO_dow['7']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Date', u'Day_of_Week'),\n",
       " (u'09/01/2014', u'5'),\n",
       " (u'20/01/2014', u'2'),\n",
       " (u'21/01/2014', u'3'),\n",
       " (u'15/01/2014', u'4')]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets not forget what our Pair RDD look like\n",
    "accDow.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u' Friday', 1), (u' Tuesday', 1), (u' Wednesday', 1), (u' Thursday', 1)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will now use our lookup RDD to get the days and assign number 1 to each\n",
    "accDow = accDow.filter(lambda x : 'Date' not in x).map(lambda x: (lookupISO_dow[x[1]], 1 ))\n",
    "accDow.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u' Saturday', 23960),\n",
       " (u' Wednesday', 22318),\n",
       " (u' Thursday', 22210),\n",
       " (u' Friday', 21780),\n",
       " (u' Tuesday', 21093),\n",
       " (u' Sunday', 19021),\n",
       " (u' Monday', 15940)]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#Lets sum up all the values, and sort in descending order to get the highest accident rate by Day of the Week\n",
    "aggregate_data = accDow.reduceByKey(lambda x, y: x + y).sortBy(lambda x: -x[1]).persist()\n",
    "aggregate_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.plt?\n",
    "#dir(sns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, axes = sns.plt.subplots(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axes[0, 0].set_title('Days_Of_Week_Accidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Taken from http://stackoverflow.com/questions/26171230/matplotlib-seaborn-barplot-strings-in-x-axis   \n",
    "#axes.factorplot(\"age\", \"jobs\", col=\"industry\", row=\"city\", data=df_city,\n",
    "               #margin_titles=True, size=3, aspect=.8, palette=[\"darkred\"])\n",
    "    \n",
    "axes[0,0].plot([1,4,5,4,5,6,4,6,], label = 'DOW Accidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#.axes[0,1].clear()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.factorplot(\"DOW\", \"# of Accidents\", col=\"DOW\", row=\"Accidents\", data=aggAcc,\n",
    " #              margin_titles=True, size=3, aspect=.8, palette=[\"darkred\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have this information, we should automate the task to take input for different years and load the data into HIVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next is to analyze the average accidents by speed limit\n",
    "accSpeedLimit = parsedAccData.map(lambda x: (x.Speed_limit)).filter(lambda x: 'Speed_limit' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'30', u'30', u'30', u'30', u'30']"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accSpeedLimit.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146322"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of accidents in the year\n",
    "accSpeedLimit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Road accidents with the highest number based on Speed Limit\n",
    "#Next is to analyze the average accidents by speed limit. \n",
    "#sumCountPerLimit = accSpeedLimit.aggregate\n",
    "accSpeedLimit_SumCount = accSpeedLimit.map(lambda x: (x, 1)).combineByKey(lambda x:(x, 1),                 #Combiner function  \n",
    "                                          (lambda acc, x: (acc[0] + x, acc[1] + 1)),                          #Merge Function \n",
    "                                          (lambda acc1, acc2 :(acc1[0] + acc2[0], acc1[1] + acc2[1])))           #MergeCombiners Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accSpeedLimit_SumCount = accSpeedLimit_SumCount.collect()\n",
    "#accSpeedLimit_SumCount\n",
    "\n",
    "\n",
    "accCountPerSpeedLimit = accSpeedLimit.map(lambda x: (x , 1)).reduceByKey(lambda x , y: x + y)\n",
    "\n",
    "accCountPerSpeedLimit = accCountPerSpeedLimit.collect()\n",
    "accCount = accSpeedLimit.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.67941936277525, 2.3359440138871803, 8.212708956957943, 64.92325145911073, 3.9618102540971285, 6.886865953171773]\n"
     ]
    }
   ],
   "source": [
    "#Need some tidying up here, but the output gives us the average accidents happening per speed limit.\n",
    "avgAccPerSpeedLimit = []\n",
    "accCount = float(accCount)\n",
    "for i in accCountPerSpeedLimit:\n",
    "    avg = (i[1]/accCount) *100\n",
    "    avgAccPerSpeedLimit.append(avg)\n",
    "    #out = 'Average Accidents is { :.2f}'.format(avgAccPerSpeedLimit)\n",
    "print(avgAccPerSpeedLimit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Will use this Soon\n",
    "accSpeedLimit_SumCount = accSpeedLimit.combineByKey(lambda x:(x, 1),                 #Combiner function  \n",
    "                                          (lambda acc, x: (acc[0] + x, acc[1] + 1)),                          #Merge Function \n",
    "                                          (lambda acc1, acc2 :(acc1[0] + acc2[0], acc1[1] + acc2[1]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
